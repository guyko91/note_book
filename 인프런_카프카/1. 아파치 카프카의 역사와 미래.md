
## 카프카의 탄생과 카프카의 내부 구조

* 카프카는 링크드인에서 파편화된 데이터를 처리하기 위해 만들어졌다.
* 카프카 내부는 여러개의 토픽(topic)으로 구성되어 있다.
	* 각 토픽은 N 개의 파티션으로 구성되어 있다.
	* 파티션은 FIFO 방식의 큐(Queue) 자료구조와 유사하다.
* 어떤 특정 데이터를 보내는 client 가 '프로듀서' 이고, 큐에서 데이터를 가져가는 것이 '컨슈머' 다.
* 프로듀서가 보낸 데이터가 모든 파티션에 저장되는 것은 아니고, 여러 파티션 중 한곳에 저장된다.
* 컨슈머가 데이터를 가져 가더라도, 파티션의 **데이터는 삭제되지 않는다.**
	* 커밋(commit) 을 통해서 내부적으로 어디까지 데이터를 가져갔는지 기록하고 있다.

---

## 아파치 카프카가 데이터 파이프라인으로 적합한 4가지 이유

### 1. 높은 처리량

* 프로듀서가 브로커로 보낼때, 컨슈머가 브로커로부터 데이터를 받을 때, 모두 데이터를 묶어서 전송한다. 
	* 데이터를 묶어서 보냄으로써, 통신 횟수를 최소화 하고 동일 시간 내에 더 많은 데이터를 전송할 수 있다.
* 데이터 처리량을 늘리려면
	* 파티션 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리량을 늘리는 것 (scale-out)
	* 파티션과 컨슈머가 1개일때보다 5개씩 배포 시 처리량이 5배로 증가한다.

## 2. 확장성

* 데이터 파이프라인에서 데이터를 모을 때 데이터가 얼마나 들어올지 예측할 수 없다.
* 보통 카프카는 구성 시, 최소 2개 이상의 카프카 브로커를 클러스터링으로 구성한다.
	* 각 브로커별로 처리량은 한계가 있다. 필요 시, 브로커 개수를 조절하여 scalable 하게 대응이 가능하다. (scale-out)
	* 반대로 브로커에 대해서 scale-in 도 가능하다. 
	* 이 모든 작업은 365일 24시간 무중단으로 처리가 가능하다.

## 3. 영속성

* 영속성 = 프로그램이 종료되더라도 데이터가 사라지지 않는 특성.
* 카프카는 데이터를 메모리에 저장하지 않고, 파일 시스템에 저장한다.
	* 페이지 캐시 메모리 영역을 활용하여 한번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식으로 되어 있다.
	* 디스크 기반의 파일 시스템을 활용한 덕분에 카프카 어플리케이션이 종료되더라도 재시작 시 안전하게 데이터를 다시 처리할 수 있다.

## 4. 고가용성

* 카프카 클러스터는 브로커라고 불리는 프로세스를 3개 이상 실행시켜 운영한다.
	* 클러스터링 되어 있는 브로커 끼리는 서로의 데이터가 복제(replication) 되어 다른 브로커 프로세스에도 적재된다.
* 리전단위의 장애(서버랙 또는 퍼블릭 클라우드의 리전 등)에도 데이터를 안전하게 복제할 수 있는 브로커 옵션들이 존재한다. 

---

## 빅데이터 아키텍처의 종류와 카프카의 미래

#### 1. 빅데이터의 역사
* 과거
	* 원천 데이터 -> 파생 데이터 -> 서빙 데이터
	* 배치를 통해서 각 서비스 애플리케이션으로부터 데이터를 모았다. 
	* 원천 데이터(애플리케이션 데이터)로부터 파생된 데이터의 히스토리를 파악하기가 어려웠고, 계속되는 데이터 가공으로 데이터가 파편화 되면서 데이터 거버넌스를 지키기 어려웠다.
* 과도기1 : "람다 아키텍처"
	* 3가지 레이어로 구분한다.
		* 배치 레이어 : 배치 데이터를 모아서 특정 시간, 타이밍마다 일괄 처리한다.
		* 스피드 레이어(카프카) : 서비스에서 생성된 원천 데이터를 실시간으로 분석하는 용도로 사용한다.
		* 서빙 레이어 : 배치 레이어 및 스피드 레이어를 통해 가공된 데이터를 사용자, 서비스 애플리케이션이 사용할 수 있도록 저장된 데이터 레이어이다.
	* 람다 아키텍처의 한계
		* 데이터를 처리하는 레이어가 배치 레이어와 스피드 레이어 두개로 나뉘어지기 때문에, 데이터를 분석, 처리하는 데에 필요한 로직이 2벌로 각각의 레이어에 따로 존재해야 한다는 점.
		* 배치 데이터와 실시간 데이터를 융합하여 처리할 때는 다소 유연하지 못한 파이프라인을 생성해야 한다는 점.
* 과도기2 : "카파 아키텍처"
	* 람다 아키텍처에서 배치 레이어가 사라진 형태. (람다 아키텍처의 단점을 해소하기 위해 제이 크렙스가 제안한 아키텍처.)
	* 로직의 파편화, 디버깅, 배포, 운영 분리에 대한 이슈를 제거하기 위해 배치 레이어를 제거했다. 
	* 스피드 레이어 내에서 데이터를 모두 처리할 수 있게 되어 데이터 엔지니어들이 더욱 효율적으로 개발과 운영에 임할 수 있게 됐다.
* 가까운 미래 : "스트리밍 데이터 레이크"
	* 2020년 카프카 서밋에서 제이크렙스가 제안.
	* 서빙 레이어도 삭제된, 스피드 레이어만 존재하는 아키텍처.
	* 카프카에 분석과 프로세싱을 완료한 거대한 용량의 데이터를 오랜 기간 저장하고 사용할 수 있다면 서빙레이어는 제거되어도 된다.
	* 자주 사용하는 데이터와 자주 접근하지 않는 데이터를 구분하여 저장하는 방식을 고려하고 있다. (자주 접근하지 않는 데이터는 오브젝트 스토리지에 저장, 자주 사용하는 데이터는 메모리, 디스크에 올리는 등) -> 티어드 스토리지 (teared storage)

#### 2. 배치 데이터와 스트림 데이터 ?

* 배치 데이터
	* 한정된(bounded) 데이터 처리
	* 대규모 배치 데이터를 위한 분산 처리 수행
	* 분, 시간, 일 단위 처리를 위한 지연 발생
	* 복잡한 키 조인 수행
* 스트림 데이터
	* 무한(unbounded) 데이터 처리
	* 지속적으로 들어오는 데이터를 위한 분산 처리 수행
	* 분 단위 이하 지연 발생
	* 단순한 키 조인 수행